---
layout: post
title: "RocketMQ原理-消费者Reblance"
date:   2022-09-13
tags: [RocketMQ]
comments: true
author: xy
# toc: true  # 这里加目录导致看到地方太小了
---

本文主要内容如下:

1. `RocketMQ` 消费者`Reblance`基本流程和原理

2. `RocketMQ` `Reblance`队列分配策略

3. 增减`consumer client` `broker`主动推送通知`client reblance`

4. 当前`topic`下的队列发送变化触发`reblance`流程


### 1. `RocketMQ` 消费者`Reblance`基本流程和原理

`Rebalance`(再均衡)机制指的是：将一个`Topic`下的多个队列(或称之为分区)，在同一个消费者组(`consumer group`)下的多个消费者实例(`consumer instance`)之间进行重新分配。 目的是为了提升消息的并行处理能力。

Rebalance可能带来的问题:

1. `消费暂停`：考虑在只有Consumer 1的情况下，其负责消费所有5个队列；在新增Consumer 2，触发Rebalance时，需要分配2个队列给其消费。那么Consumer 1就需要停止这2个队列的消费，等到这两个队列分配给Consumer 2后，这两个队列才能继续被消费。

2. `重复消费`：Consumer 2 在消费分配给自己的2个队列时，必须接着从Consumer 1之前已经消费到的offset继续开始消费。然而默认情况下，offset是异步提交的，如consumer 1当前消费到offset为10，但是异步提交给broker的offset为8；那么如果consumer 2从8的offset开始消费，那么就会有2条消息重复。

3. `消费堆积`：由于rebalance可能导致重复消费，如果需要重复消费的消息过多；或者因为rebalance暂停时间过长，导致积压了部分消息。那么都有可能导致在rebalance结束之后瞬间可能需要消费很多消息。一般提前评估好消息的生产速度和消费速度。

触发`Rebalance`的根本因素有两个：

1. 订阅Topic的队列数量变化(broker宕机broker升级等运维操作队列扩容/缩容)

2. 消费者组信息变化(例如发版的时候服务上下线, 消费者网络异常, consume就会增减)


### 2. `RocketMQ` `Reblance`队列分配策略

`RocketMQ`默认使用的是平均负载策略`AllocateMessageQueueAveragely`, 如果某个`Consumer`集群，订阅了某个`Topic`，`Topic`下面的这些`MessageQueue`会被平均分配给集群中的`Consumer`。例如下图:

![img](../images/rocketmq.assests/consumer_blance_1.png)

上面的图应该很好理解了, 需要注意的是, 当`MessageQueue`数量小于`Consumer`数量会有`Consumer`拿不到`Queue`。

例如上图`C5`排在最后面，队列全被别人拿走了，`C5`就一直分不到消息队列，除非前面的某个`Consumer`挂了，20s之后，在队列重新负载的时候就能拿到`MessageQueue`。

分配策略的接口定义如下:

```java
public interface AllocateMessageQueueStrategy{
    // 返回当前clientid被分配的 队列列表
    List<MessageQueue> allocate(
            String consumerGroup,
            String currentCID,              /*当前的client id*/
            List<MessageQueue> mqAll,       /*所有的队列*/
            List<String> cidAll             /*所有的clientid*/
    );
}
```

平均策略实现如下:

```java
public class AllocateMessageQueueAveragely implements AllocateMessageQueueStrategy {
@Override
List<MessageQueue> allocate(consumerGroup, currentCID, List<MessageQueue> mqAll,List<String> cidAll){
    List<MessageQueue> result = new ArrayList<MessageQueue>();
    int index = cidAll.indexOf(currentCID);
    int mod = mqAll.size() % cidAll.size();
    ...
    int range = Math.min(averageSize, mqAll.size() - startIndex);
    for (int i = 0; i < range; i++) {
        result.add(mqAll.get((startIndex + i) % mqAll.size()));
    }
    return result;
}
}
```

相同`group`下的所有`consumer client`都用用一种分配策略这样一般同一个队列在集群模式下就只会被一个`consumer`消费了。

还有其他策略, 例如 机房负载策略AllocateMessageQueueByMachineRoom当前Consumer只负载处在指定的机房内的MessageQueue。 AllocateMachineRoomNearby同机房的改进版本(这里简单带过下, 后面需要深入的时候再看)。

### 3. 增减`consumer client` `broker`主动推送通知`client reblance`

`RocketMQ`支持双向通信机制，在客户端通过`ClientRemotingProcessor`的`processRequest`方法来处理`Broker`发起的通知请求。`broker`收到消费者`client`的注册和取消注册事件会通知消费者执行`reblance`:

```java
public class ConsumerManager{
private final ConcurrentMap<String/* Group */, ConsumerGroupInfo> consumerTable = new..;
public boolean registerConsumer(final String group, final ClientChannelInfo clientChannelInfo..){
    boolean r1 = consumerGroupInfo.updateChannel();
    boolean r2 = consumerGroupInfo.updateSubscription(subList);
    if (r1 || r2){
        this.consumerIdsChangeListener.handle(ConsumerGroupEvent.CHANGE, group);
    }
    ...
}
public void unregisterConsumer(){
    if(changed){
        consumerIdsChangeListener.handle(ConsumerGroupEvent.CHANGE, group...);
    }......
}
}
```

然后调用通知`client`消费者队列改变了:

```java
public class DefaultConsumerIdsChangeListener{
@Override
public void handle(ConsumerGroupEvent event, String group, Object... args){
    switch (event){
        case CHANGE:
            this.brokerController.getBroker2Client().notifyConsumerIdsChanged(chl, group);
public class Broker2Client{
    public void notifyConsumerIdsChanged(...){
        RemotingCommand request = RemotingCommand.createRequestCommand(...);
        this.brokerController.getRemotingServer().invokeOneway(channel, request, 10);
   ....
```

`client`收到请求之后, 立马执行`reblance`：

```java
public class ClientRemotingProcessor extends AsyncNettyRequestProcessor ..{
@Override
public RemotingCommand processRequest(ChannelHandlerContext ctx, request){
    switch (request.getCode()) {
        case RequestCode.NOTIFY_CONSUMER_IDS_CHANGED:   消费者数量变化通知
            return this.notifyConsumerIdsChanged(ctx, request);    
    ...
    }
}
public RemotingCommand notifyConsumerIdsChanged(ChannelHandlerContext ctx,request){
    ...    
    NotifyConsumerIdsChangedRequestHeader requestHeader = request.decodeCommandCustomHeader();
    this.mqClientFactory.rebalanceImmediately();
    ...
}  
}
```

### 3. 当前`topic`下的队列发送变化触发`reblance`流程

`consume`在启动的时候会启动定时任务, 拉取订阅的`topic`下的队列是否发生变化:

```java
public class DefaultLitePullConsumerImpl implements MQConsumerInner{
    public synchronized void start(){
        ...
        startScheduleTask();
    }
    private void startScheduleTask() {
        scheduledExecutorService.scheduleAtFixedRate(
            new Runnable() {
                @Override
                public void run() {
                    fetchTopicMessageQueuesAndCompare();
                }
            }, 1000 * 10, getTopicMetadataCheckIntervalMillis();
    }
}
```

对比逻辑就是对比当前`consume`订阅的所有`topic`的`queue`是否发送变化:


```java
private synchronized void fetchTopicMessageQueuesAndCompare() throws MQClientException {
    for (Map.Entry<String, TopicMessageQueueChangeListener> entry : topicMessageQueueChangeListenerMap.entrySet()) {
        String topic = entry.getKey();
        TopicMessageQueueChangeListener topicMessageQueueChangeListener = entry.getValue();
        Set<MessageQueue> oldMessageQueues = messageQueuesForTopic.get(topic);
        Set<MessageQueue> newMessageQueues = fetchMessageQueues(topic);
        boolean isChanged = !isSetEqual(newMessageQueues, oldMessageQueues);
        if (isChanged) {
            messageQueuesForTopic.put(topic, newMessageQueues);
            if (topicMessageQueueChangeListener != null) {
                topicMessageQueueChangeListener.onChanged(topic, newMessageQueues);
```

队列数量变化的处理接口定义如下:

```java
public interface MessageQueueListener {
void messageQueueChanged(String topic, final Set<MessageQueue> mqAll /*所有的队列*/,
                         Set<MessageQueue> mqDivided /*分配的队列*/);
}
```

`Pull consume`定义是实现如下(广播模式重新拉取全量的队列, 集群模式拉取新分配的对列)：

```java
class MessageQueueListenerImpl implements MessageQueueListener {
@Override
public void messageQueueChanged(String topic, mqAll, mqDivided) {
    MessageModel messageModel = defaultLitePullConsumer.getMessageModel();
    switch (messageModel) {
        case BROADCASTING:
            updateAssignedMessageQueue(topic, mqAll);
            updatePullTask(topic, mqAll);
            break;
        case CLUSTERING:
            updateAssignedMessageQueue(topic, mqDivided);
            updatePullTask(topic, mqDivided);
            ...
    }
}
}
```

如果已经分配的队列现在不能处理了, 就设置为droped, 消费缓存队列里面会跳过这些droped队列:

```java
public void updateAssignedMessageQueue(String topic, Collection<MessageQueue> assigned) {
    synchronized (this.assignedMessageQueueState) {
        Iterator<Map.Entry<MessageQueue, MessageQueueState>> it = this.assignedMessageQueueState...;
        while (it.hasNext()) {
            Map.Entry<MessageQueue, MessageQueueState> next = it.next();
            if (next.getKey().getTopic().equals(topic)) {
                if (!assigned.contains(next.getKey())) {
                    next.getValue().getProcessQueue().setDropped(true);  队列丢弃掉
                    it.remove();
                ........
        addAssignedMessageQueue(assigned);
```

### 参考

https://blog.csdn.net/qq_38082304/article/details/112378245
